# 3.3장 쿠버네티스 연결을 담당하는 서비스

## 스터디 날짜
2022/03/03

쿠버네티스 클러스터 내부에만 파드를 이용하는건가?<br>
아니다. 이번에는 외부 사용자가 파드를 이용하는 방법을 알아보자<br>

**서비스(service)** : 쿠버네티스에서는 외부에서 쿠버네티스 클러스터에 접속하는 방법을 서비스라고 한다.
## 3.3.1 가장 간단하게 연결하는 노드포트

노드포트(Nodeport)
- 외부에서 쿠버네티스 클러스터의 내부에 접속하는 가장 쉬운 방법
- 노드포트 서비스 설정 시 모든 워커 노드의 특정 포트(노드포트)를 열고 여기로 오는 모든 요청을 노드포트 서비스로 전달
- 노드포트 서비스는 해당 업무를 처리할 수 있는 파드로 요청전달


![3_35](https://user-images.githubusercontent.com/71916223/156020507-be016567-cf53-417f-975b-15d5ad63520a.PNG)
<br>

### 노드포드 서비스로 외부에서 접속히기
1. 디플로이먼트로 파드 생성
```shell
kubectl create deployment np-pods --image=sysnet4admin/echo-hname
```
2. kubectl create 로 노드포트 서비스 생성
```shell
kubectl create -f ~/_Book_k8sInfra/ch3/3.3.1/nodeport.yaml
```

**nodeport.yaml**
```yaml
apiVersion: v1
kind: Service # kind 가 Service 로 바뀌었다.
metadata:
  name: np-svc # 서비스의 이름
spec: # 컨테이너에 대한 정보는 없다.
  selector: # 설렉터의 레이블 지정
    app: np-pods 
  ports: # 사용할 프로토콜과 포트들을 지정
    - name: http
      protocol: TCP
      port: 80
      targetPort: 80
      nodePort: 30000
  type: NodePort # 서비스 타입을 설정
```

3. 노드포트 서비스로 생성한 np-svc 서비스 확인
```shell
kubectl get services
```
Cluster-IP : 쿠버네티스 클러스터의 내부에서 사용하는 IP로, 자동으로 지정<br>
노드포트의 포트 번호가 30000번으로 지정되었다.<br>

4. 쿠버네티스 워커노드 IP 확인
```shell
kubectl get nodes -o wide
```

5. 웹 브라우저를 띄우고 192.168.1.101 ~ 103(확인한 워커노드의 IP)와 30000번(노드포트의 포트 번호) 으로 접속해 외부에서 접속되는지 확인한다<br>
<br>
이때 파드가 1개 이므로 화면에 보이는 이름은 모두 동일하다<br>

**192.169.1.101:30000**<br> 
**192.169.1.102:30000**<br>
**192.169.1.103:30000**<br>
화면에 보이는 이름은 모두 동일한 파드 이름<br>
<br>
배포된 파드에 모든 노드의 노드포트를 통해 외부에서도 접속가능함을 확인함

### 부하 분산 테스트하기

1. powershell 명령 창을 띄우고 명령 실행. 반복적으로 192.168.1.101:30000 에 접속해 접속한 파드 이름을 화면에 표시(Invoke-RestMethod).
```shell
$i=0; while($true)
{
  % { $i++; write-host -NoNewline "$i $_"}
  (Invoke-RestMethod "http://192.168.1.101:30000")-replace '\n', " "
}
```

2. 쿠버네티스 마스터 노드에서 scale 을 통해 파드를 3개로 증가
```shell
kubectl scale deployment np-pods --replicas=3
```

3. powershell 명령 창에 늘어난 3개의 파드 이름을 돌아가면서 표시되는것을 확인할 수 있다. 부하 분산이 제대로 이루어졌다.<br>
<br>
추가되 파드를 외부에서 추적해 접속하는 방식은 노드포트의 오브젝트 스펙에 적힌 np-pods 와 디플로이먼트의 이름을 확인해 동일하면 같은 파드라고 간주하기 때문이다.<br>
추적 방법을 많지만, 여기서는 이름으로 진행하였다.

**nodeport.yaml 파일 일부**
```shell
spec: # 컨테이너에 대한 정보는 없다.
  selector: # 설렉터의 레이블 지정
    app: np-pods 
```

### expose 로 노드포트 서비스 생성
오브젝트 스펙 파일 뿐만아니라 expose 명령어를 통해 노드포트 서비스를 생성할 수 있다.<br>
1. expose 명령어로 서비스로 내보낼 디플로이먼트 지정
```shell
kubectl expose deployment np-pods --type=NodePort --name=np-svc-v2 --port=80
```
서비스로 내보낼 디플로이먼트 이름 : np-pods<br>
서비스 이름 : np-svc-v2<br>
타입 : NodePort(서비스 타입은 반드시 대소문자 구분)<br>
서비스가 파드로 보내줄 연결 포트 : 80<br>
<br>
2. 서비스 확인
```shell
kubectl get services
```
expose 사용시 노드포트의 포트 번호를 지정할 수 없다. 30000~32767 에서 임의로 지정된다.<br>
**192.168.1.101:32122**<br>

3. 삭제
```shell
kubectl delete deployment np-pods
kubectl delete services np-svc
kubectl delete services np-svc-v2
```

## 3.3.2 사용 목적별로 연결하는 인그레스
노드포트 서비스는 포트를 중복 사용할 수 없어서 1개의 노트포트에 1개의 디플로이먼트만 작동된다.<br>
여러개의 디플로이먼트가 있을 경우 그 수만큼 노드포트 섭스를 구동해야 할까?<br>
이런 경우 인그레스를 사용한다<br>
<br>
**인그레스(Ingress)** : 고유한 주소를 제공해 사용 목적에 따라 다른 응답을 제공할 수 있고, 트래픽에 대한 L4/L7 로드밸런서와 보안 인증서를 처리하는 기능을 제공합니다.
인그레스를 사용하기 위해 **인그레스 컨트롤러** 필요.<br>
다양한 인그레스 컨트롤러 중에 여기서는 쿠버네티스에서 프로젝트로 지원하는 **NGINX 인그레스 컨트롤러** 로 구성<br>
<br>
NGNIX 인그레스 컨트롤러 작동 단계
1. 사용자는 노드마다 설정된 노드포트를 통해 노드포트 서비스로 접속한다. 이때 노드포트 서비스를 NGNIX 인그레스 컨트롤러로 구성한다.
2. NGINX 인그레스 컨트롤러는 사용자이 접속 경로에 따라 적합한 클러스터 IP 서비스로 경로를 제공한다
3. 클러스터 IP 서비스는 사용자를 해당 파드로 연결해준다.

* 인그레스 컨트롤러는 파드와 직접 통신할 수 없어서 노드포트 또는 로드밸런서 서비스와 연동되어야 한다. 따라서 노드포트로 연동했다.*<br>
<br>

![3_41](https://user-images.githubusercontent.com/71916223/156020511-2fe22a21-e4b6-4c6e-95fc-24a96e90065a.PNG)
<br>
1. 테스트용으로 디플로이먼트 2개를 배포
```shell
kubectl create deployment in-hname-pod --image=sysnet4admin/echo-hname
kubectl create deployment in-ip-pod --image=sysnet4admin/echo-ip
```
2. NGINX 인그레스 컨트롤러를 설치한다.(미리 구성들이 지정되어있다)
```shell
kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress-nginx.yaml
```
3. NGINX 인그레스 컨트롤러의 파드 배포 확인. NGINX 컨트롤러는 default 가 아닌 ingress-nginx 네임스페이스에 속해서 ```-n ingress-nginx``` 를 추가한다.-n 은 네임스페이스의 약자로 default 이외의 네임스페이스를 확인할 때 사용하는 옵션이다. 파드뿐 아니라 서비스확인 시도 동일한 옵션을 준다
```shell
kubectl get pods -n ingress-nginx
```
4. 인그레스를 사용자 요구 사항에 맞게 설정하려면 경로와 작동을 정의해야 한다. 파일로 설정
```shell
kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress-config.yaml
```
이 파일은 들어오는 주소 값과 포트에 따라 노출된 서비스를 연결하는 역할을 설정한다. 외부에서 주소 값과 노드포트를 가지고 들어오는 것은 hname-svc-default 서비스와 연결된 파드로 넘기고, 외부에서 들어오는 주소 값, 노드포트와 함께 뒤에 /ip 를 추가한 주소 값은 ip-svc 서비스와 연결된 파드로 젒고하세 설정한다.
```yaml
apiVersion: networking.k8s.io/v1beta1
kind: Ingress
metadata:
  name: ingress-nginx
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path:
        backend:
          serviceName: hname-svc-default
          servicePort: 80
      - path: /ip
        backend:
          serviceName: ip-svc
          servicePort: 80
      - path: /your-directory
        backend:
          serviceName: your-svc
          servicePort: 80

```

![3_42](https://user-images.githubusercontent.com/71916223/156020515-57700f39-ca6a-4c09-ab54-84dfb46beac2.PNG)
<br>
5. 인그레스 설정 파일이 제대로 등록외었는지 확인
```shell
kubectl get ingress
```
6. 인그레스에 요천한 내용이 확실하게 적용되었는지 확인. 우리가 적용된 내용 외에도 시스템에서 자동으로 생성해준것까지 확인가능
```shell
kubectl get ingress -o yaml
```
7. NGINX 인그레스 컨트롤러 생성과 인그레스 설정 환요. 외부에서 NGINX 인그레스 컨트롤러에 접속할 수 있게 노드포트 서비스로 NGINX 인그레스 컨트롤러를 외부에 노출한다.
```shell
kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.2/ingress.yaml
```
기존 노드포트와 달리 http 를 처리하기 위해 30100 번 포트로 들어온 요청을 80번 포트로 넘기고 https 를 처리하기 위해 30101 번 포트로 들어온것을 443번 포트로 넘긴다. NGIX 인그레스 컨트롤러가 위치하는 네임스페이스를 ingress-nginx 로 지정하고 NGINX 인그레스 컨트롤러의 요구 사항에 따라 셀렉터를 ingress-nginx 로 지정했다
```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress-controller
  namespace: ingress-nginx
spec:
  ports:
  - name: http
    protocol: TCP
    port: 80
    targetPort: 80
    nodePort: 30100
  - name: https
    protocol: TCP
    port: 443
    targetPort: 443
    nodePort: 30101
  selector:
    app.kubernetes.io/name: ingress-nginx
  type: NodePort
```

![image](https://user-images.githubusercontent.com/71916223/156021077-e5661e70-227b-4530-969a-ec19a79788df.png)
<br>

8. 노드포트 서비스로 생성된 NGINX 인그레스 컨트롤러를 확인한다. 
```shell
kubectl get services -n ingress-nginx
```

9. expose 명령으로 디플로이먼트(in-hname-pod, in-ip-pid)도 서비스로 노출한다. 외부와 통신하기 위해 클러스터 내부에서만 사용하는 파드를 클러스터 외부에 노출할 수 있는 구역으로 옮기는 것이다
```shell
kubectl expose deployment in-hname-pod --name=hname-svc-default --port=80,443
kubectl expose deployment in-ip-pod --name=ip-svc --port=80,443
```

10. 생성된 서비스를 점검해 디플로이먼트들이 서비스에 정상적으로 노출되는지 확인. 새로 생성된 서비스는 default 에 있으므로 -n 옵션 주지 않는다.
```shell
kubectl get services
```

11. 실습해보자
**192.168.1.101.30100 접속시 in-hname-pod 확인가능**<br>
**192.168.1.101.30100/ip 접속시 request_method 확인가능**<br>
**https://192.168.1.101.30100 접속시 in-hname-pod 확인가능**<br>
**https://192.168.1.101.30100/ip 접속시 request_method 확인가능**<br>

12. 삭제
```shell
kubectl delete deployment in-hname-pod
kubectl delete deployment in-ip-pod
kubectl delete services hname-svc-default
kubectl delete services ip-svc

kubectl delete -f ~/_Book_k8sInfra/ch3/3.3.2/ingress-config.yaml
```

## 3.3.3 클라우드에서 쉽게 구성 가능한 로드밸런서
앞에서 배운 연결 방식은 들어오는 요청을 모두 워커 노드의 노드포트를 통해 노드포트 서비스로 이동하고 이를 다시 쿠버네티스의 파드로 보내는 구조이다. 매우 비효율적이다<br>
쿠버네티스는 로드밸런서(LoadBalancer)라는 서비스 타입을 제공해 간단한 구조로 파드를 외부에 노출하고 부하를 분산한다.

![3_48](https://user-images.githubusercontent.com/71916223/156020517-b1302ec5-4b09-4e39-a87f-96f3fd6aaa83.PNG)
<br>
로드밸런서를 사용하려면 로드밸런서를 이미 구현해 둔 서비스업체의 도움을 받아 쿠버네티스 클러스터 외부에 구현해야 한다.

### 3.3.4 온프레미스 로드밸런서를 제공하는 MetalLB
온프레미스에서 로드밸런서를 사용하려면 내부에 로드밸런서 서비스를 받아주는 구성이 필요하고 이를 지원하는 것이 MetalLB 이다. 기존의 L2 네트워크와 L3 네트워크로 로드밸런서를 구현한다.

![3_49](https://user-images.githubusercontent.com/71916223/156020519-48545057-e0c1-4a64-a481-31aca273a43b.PNG)
<br>
MetalLB 컨트롤러 : 작동방식(Protocol)을 정의하고 EXTERNAL-IP 를 부여해 관리한다.<br>
MetalLB 스피커 : 정해진 작동방식(L2/ARP, L3/BFP) 에 따라 경로를 만들 수 있도록 네트워크 정보를 광고하고 수집해 각 파드의 경로를 제공한다.<br>
이때 L2 는 스피커 중에서 리더를 선출해 경로 제공을 총괄하게 한다.

1. 디플로이먼트를 이용해 2종휴의 파드를 생성한다. 그 후 scale 명령으로 파드를 3개로 늘려 노드당 1개씩 파드가 배포되게 한다.
```shell
kubectl create deployment lb-hname-pods --image=sysnet4admin/echo-hname
kubectl scale deployment lb-hname-pods --replicas=3
kubectl create deployment lb-ip-pods --image=sysnet4admin/echo-ip
kubectl scale deployment lb-ip-pods --replicas=3
```

2. 2종류의 파드가 3개씩 총 6개 배포됨을 확인한다
```shell
kubectl get pods
```

3. 인그레스와 마찬가지로 사전에 정의된 오브젝트 스펙으로 MetalLB 를 구성한다. 이렇게 하면 MetalLB 에 필요한 요소가 모두 설치되고 독립적인 네임스페이스(metallb-system)도 함께 만들어진다.
```shell
kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb.yaml
```
4. 배포된 MetalLB 의 파드가 5개(controller 1개, speaker 4개)인지 확인하고, IP 와 상태도 확인한다
```shell
kubectl get pods -n metallb-system -o wide
```

5. MetalLB 도 설정을 적용한다. 오브젝트는 ConfigMap 을 이용한다. 설정이 정의된 포맷을 의미한다
```shell
kubectl apply -f ~/_Book_k8sInfra/ch3/3.3.4/metallb-l2config.yaml
```

**metallb-l2config.yaml**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: nginx-ip-range
      protocol: layer2
      addresses:
      - 192.168.1.11-192.168.1.13
```

![3_50](https://user-images.githubusercontent.com/71916223/156020497-ff95c9bb-c11f-4081-b64b-a0012b4bf07c.PNG)
<br>

6. ConfigMap 이 생성되었는지 확인
```shell
kubectl get configmap -n metallb-system
```

7. -o yaml 옵션으로 vi 확인
```shell
kubectl get configmap -n metallb-system -o yaml
```

8. 모든 설정 완료. 각 디플로이먼트를 로드밸런서 서비스로 노출
```shell
kubectl expose deployment lb-hname-pods --type=LoadBalancer --name=lb-hname-svc --port=80
kubectl expose deployment lb-ip-pods --type=LoadBalancer --name=lb-ip-svc --port=80
```
9. 생성된 로드밸런서 서비스별로 CLUSTER_IP 와 EXTERNAL-IP 가 잘 적용되었는지 확인. 특히 EXTERNAL_IP 에 ConfigMap 을 통해 부여한 IP 확인
```shell
kubectl get services
```

10. EXTERNAL-IP 로 잘 작동되는지 확인
**192.168.1.11** 에 접속

11. 파워 셸 작성
```shell
$i=0; while($true)
{
  % {$i++; write-host -NoNewline "$i $_"}
  (Invoke-RestMethod "http://192.168.1.11")-replace '\n', " "
}
```

12. scale 명령으로 파드를 6개로 늘린다
```shell
kubectl scale deployment lb-hname-pods --replicas=6
```

13. 늘어난 파드 6개도 EXTERNAL-IP를 통해 접근이 된다.

14. 삭제
```shell
kubectl delete deployment lb-hname-pods
kubectl delete deployment lb-ip-pods
kubectl delete service lb-hname-pods
kubectl delete service lb-ip-svc
```

## 3.3.5 부하에 따라 자동으로 파드 수를 조절하는 HPA
사용자가 갑자기 늘어나 파드가 더이상 감당할 수 없어서 서비스 불가라는 결과를 초래할 수도 있다. 부하량에 따라 디플로이먼트 파드 수를 유동적으로 관리하는 기능이 있으며 이를 JPA(Horizontal Pod Autoscaler)라고 한다.

1. 디플로이먼트 1개 생성
```shell
kubectl create deployment hpa-hname-pods --image=sysnet4admin/echo-hname
```

2. 앞에서 MetalLB 를 구성했으므로 expose 를 실행해 hpa-hname-pods 를 로드밸런서 서비스로 바로 설정한다
```shell
kubectl expose deployment hpa-hname-pods --type=LoadBalancer --name=hpa-hname-svc --port=80
```
3. 설정된 로드밸런서 서비스와 부여된 IP 를 확인한다
```shell
kubectl get services
```

4. HPA 가 작동하려면 파드의 자원이 어느 정도 사용되는지 파악해야 한다. 부하를 확인하는 명령은 kubectl top pods 이다
```shell
kubectl top pods
// 에러발생
```
자원이 요청하는 설정이 없다고 에러가 발생. 

![3_54](https://user-images.githubusercontent.com/71916223/156020505-b3bb5c15-9263-45f1-9954-ad3c59de1663.PNG)
<br>
HPA 가 자원을 요청할 때 메트릭 서버(Metric-Server)를 통해 계측값을 전달받는다. 그런데 현재 메트릭 서버가 없기 때문에 에러발생. 메트릭 서버 설정 필요

5. 메트릭 서버도 오브젝트 스펙 파일로 설치 가능. 여기서는 기존 파일 로 사용
```shell
kubectl create -f ~/_Book_k8sInfra/ch3/3.3.5/metrics-server.yaml
```

6. 이제 top 명령어 사용가능하다. 현재 아무 부하가 없어 CPU 와 MEMORY 값이 낮게 나온다
```shell
kubectl top pods
```
현재 scale 기준 값이 설정되어 있지 않아서 파드 증설 시점을 알 수 없다. 따라서 파드에 부하가 걸리기 전에 scale 이 실행되게 디플로이먼트에 기준 값을 기록한다. 기존에 배포한 디플로이먼트 내용을 edit 으로 직접 수정한다.

7. 1000m = 1개 CPU. 10m 는 파드의 CPU 0.01 사용을 기준으로 파드 증설 설정. CPU 사용 제한은 0.05 로 설정
```shell
kubectl edit deployment hpa-hname-pods
```
```yaml
resources:
  request:
    cpu: "10m"
  limits:
    cpu: "50m"
```

8. hpa-hname-pods 에 autoscale 을 설정해서 특정 조건이 만족되는 경우 자동으로 scale 명령이 수행되게 한다. 여기서 min 은 최소 파드수, max 는 최대 파드 수이다. cpu-percent 는 CPU 사용량이 50% 를 넘게 되면 autoscale 하겠다는 의미이다.
```yaml
kubectl autoscale deployment hpa-hname-pods --min=1 --max=30 --cpu-percent=50
```

**HPA 를 통해 늘어나는 파드 수 계산 방법 : 175p 참고**<br>
**실제 테스트 과정 : 176p 참고**<br>

HPA 를 잘 활용하면 자원의 사용을 극대화 하면서 서비스 가동률을 높일 수 있다.